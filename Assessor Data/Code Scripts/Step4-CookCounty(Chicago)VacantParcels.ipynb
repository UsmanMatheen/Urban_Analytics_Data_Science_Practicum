{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed032d71-7fa9-4871-9289-cf449d62cee9",
   "metadata": {},
   "source": [
    "## Cook County Parcel Data Cleaning, Integration & Connected Communities Mapping\n",
    "\n",
    "This notebook processes multiple datasets at the parcel (PIN) level to prepare a clean, integrated view of land parcels in Cook County, with a focus on identifying those that fall within Chicago‚Äôs **Connected Communities (CC)** zoning area.\n",
    "\n",
    "### Core Dataset: `parcel_cc.csv`\n",
    "- Contains parcel identifiers (`pin`/`name`) and a key column called `inclusion_source`.\n",
    "- `inclusion_source` indicates the specific reason why a parcel falls within the Connected Communities boundary (e.g., zoning inclusion, transit accessibility, affordability overlays).\n",
    "- This file is used to flag PINs and later **merge with additional datasets** (e.g., tax records, city-owned land, vacant parcels).\n",
    "\n",
    "### Additional Steps Performed in This Notebook:\n",
    "- **Standardize and deduplicate PINs** across large datasets using `tax_year` to retain the latest parcel records.\n",
    "- **Merge and enrich** parcel data using geospatial and tabular joins.\n",
    "- **Filter** PINs that fall inside or outside the Connected Communities boundaries.\n",
    "- **Identify vacant parcels** using city-owned land and improvement status.\n",
    "- **Spatially map** the resulting vacant CC parcels against CTA bus routes, L stations, and block group boundaries using Folium and GeoPandas.\n",
    "- **Flag parcels** based on proximity to transit and inclusion in CC for further analysis or development prioritization.\n",
    "\n",
    "### ‚úÖ Final Outputs:\n",
    "- Cleaned and enriched dataset of Cook County parcels\n",
    "- Connected Communities inclusion flag per parcel\n",
    "- Interactive HTML map showing vacant parcels, transit access, and zoning overlays\n",
    "- **A CSV file containing all vacant properties along with their block group GEOIDs and CC inclusion status**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d0b090-139e-48c0-99ac-aa55a11fcf9f",
   "metadata": {},
   "source": [
    "# Parcel_CC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d287bb63-776e-4cc5-a71a-33acc7e70a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Point\n",
    "import folium\n",
    "from folium import Choropleth, GeoJson, GeoJsonTooltip\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "510fef46-9733-4e7f-b2fa-1ac0011b52ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************\n",
      "First 5 rows of the dataset:\n",
      "             name      pin10      job_no  longitude   latitude  census_tract  \\\n",
      "0  01271000040000  127100004           0 -88.168264  42.092977  1.703180e+10   \n",
      "1  01033000180000  103300018           0 -88.176387  42.140587  1.703180e+10   \n",
      "2  01271020230000  127102023           0 -88.175209  42.090937  1.703180e+10   \n",
      "3  01284100100000  128410010  2008000622 -88.186716  42.083475  1.703180e+10   \n",
      "4  01023000530000  102300053           0 -88.159604  42.141777  1.703180e+10   \n",
      "\n",
      "    tract_geoid  tract_white_perc  tract_black_perc inclusion_source  \n",
      "0  1.703180e+10          0.669399          0.014344              NaN  \n",
      "1  1.703180e+10          0.669399          0.014344              NaN  \n",
      "2  1.703180e+10          0.669399          0.014344              NaN  \n",
      "3  1.703180e+10          0.669399          0.014344              NaN  \n",
      "4  1.703180e+10          0.669399          0.014344              NaN  \n",
      "**********************\n",
      "\n",
      "Dataset info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1905645 entries, 0 to 1905644\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Dtype  \n",
      "---  ------            -----  \n",
      " 0   name              object \n",
      " 1   pin10             int64  \n",
      " 2   job_no            int64  \n",
      " 3   longitude         float64\n",
      " 4   latitude          float64\n",
      " 5   census_tract      float64\n",
      " 6   tract_geoid       float64\n",
      " 7   tract_white_perc  float64\n",
      " 8   tract_black_perc  float64\n",
      " 9   inclusion_source  object \n",
      "dtypes: float64(6), int64(2), object(2)\n",
      "memory usage: 145.4+ MB\n",
      "None\n",
      "**********************\n",
      "\n",
      "Missing values per column:\n",
      "name                     3\n",
      "pin10                    0\n",
      "job_no                   0\n",
      "longitude                0\n",
      "latitude                 0\n",
      "census_tract             1\n",
      "tract_geoid           5426\n",
      "tract_white_perc      5426\n",
      "tract_black_perc      5426\n",
      "inclusion_source    930096\n",
      "dtype: int64\n",
      "**********************\n",
      "\n",
      "Summary statistics:\n",
      "                  name         pin10        job_no     longitude  \\\n",
      "count          1905642  1.905645e+06  1.905645e+06  1.905645e+06   \n",
      "unique         1416445           NaN           NaN           NaN   \n",
      "top     17101040370000           NaN           NaN           NaN   \n",
      "freq               216           NaN           NaN           NaN   \n",
      "mean               NaN  1.730239e+09  1.712974e+08 -8.774068e+01   \n",
      "std                NaN  7.141101e+08  5.609088e+08  1.308785e-01   \n",
      "min                NaN  1.011000e+08  0.000000e+00 -8.826351e+01   \n",
      "25%                NaN  1.317121e+09  0.000000e+00 -8.779640e+01   \n",
      "50%                NaN  1.705107e+09  0.000000e+00 -8.771067e+01   \n",
      "75%                NaN  2.131215e+09  0.000000e+00 -8.765447e+01   \n",
      "max                NaN  3.332999e+09  2.022001e+09 -8.752457e+01   \n",
      "\n",
      "            latitude  census_tract   tract_geoid  tract_white_perc  \\\n",
      "count   1.905645e+06  1.905644e+06  1.900219e+06      1.900219e+06   \n",
      "unique           NaN           NaN           NaN               NaN   \n",
      "top              NaN           NaN           NaN               NaN   \n",
      "freq             NaN           NaN           NaN               NaN   \n",
      "mean    4.184539e+01  1.703161e+10  1.703160e+10      3.940865e-01   \n",
      "std     1.521127e-01  3.560671e+05  2.796687e+05      3.146430e-01   \n",
      "min     4.146976e+01  1.703101e+10  1.703101e+10      0.000000e+00   \n",
      "25%     4.173731e+01  1.703133e+10  1.703133e+10      5.789474e-02   \n",
      "50%     4.185893e+01  1.703180e+10  1.703180e+10      3.980410e-01   \n",
      "75%     4.195690e+01  1.703182e+10  1.703182e+10      6.887661e-01   \n",
      "max     4.215419e+01  1.719788e+10  1.703198e+10      9.637121e-01   \n",
      "\n",
      "        tract_black_perc inclusion_source  \n",
      "count       1.900219e+06           975549  \n",
      "unique               NaN                3  \n",
      "top                  NaN  cdot bus routes  \n",
      "freq                 NaN           450511  \n",
      "mean        3.057721e-01              NaN  \n",
      "std         3.761344e-01              NaN  \n",
      "min         0.000000e+00              NaN  \n",
      "25%         1.807851e-02              NaN  \n",
      "50%         5.875371e-02              NaN  \n",
      "75%         6.856594e-01              NaN  \n",
      "max         1.000000e+00              NaN  \n",
      "**********************\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "file_path = 'C:/Users/kaur6/Downloads/Urban Analytics/Parcel_CC.csv'\n",
    "df = pd.read_csv(file_path, low_memory=False)\n",
    "print(\"**********************\")\n",
    "# Preview the data\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "print(\"**********************\")\n",
    "# Summary info\n",
    "print(\"\\nDataset info:\")\n",
    "print(df.info())\n",
    "print(\"**********************\")\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"**********************\")\n",
    "# Summary statistics\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(df.describe(include='all'))\n",
    "print(\"**********************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c8ff9a6-9a47-4e04-9a38-918ad884ff8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Total duplicate rows based on 'name': 489199\n"
     ]
    }
   ],
   "source": [
    "# Count total rows with duplicated 'name' values (excluding the first occurrence)\n",
    "duplicate_row_count = df.duplicated(subset='name', keep='first').sum()\n",
    "print(f\"üîÅ Total duplicate rows based on 'name': {duplicate_row_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f45e2ae-d4cb-466f-9a5f-58e5e7f5cbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Final cleaned file saved.\n",
      "             name       pin10  job_no  longitude   latitude   census_tract  \\\n",
      "0  16052210320000  1605221032       0 -87.768517  41.904417  17031250600.0   \n",
      "1  16131260170000  1613126017       0 -87.701964  41.875518  17031271200.0   \n",
      "2  16243100090000  1624310009       0 -87.704923  41.852313  17031841700.0   \n",
      "3  16252180030000  1625218003       0 -87.692609  41.847523  17031301200.0   \n",
      "4  16123140420000  1612314042       0 -87.699154  41.884731  17031837100.0   \n",
      "\n",
      "     tract_geoid  tract_white_perc  tract_black_perc  \\\n",
      "0  17031250600.0          0.012864          0.863835   \n",
      "1  17031271200.0          0.059382          0.893112   \n",
      "2  17031841700.0          0.050959          0.438904   \n",
      "3  17031301200.0          0.031319          0.066308   \n",
      "4  17031837100.0          0.181078          0.652256   \n",
      "\n",
      "                   inclusion_source  \n",
      "0                   cdot bus routes  \n",
      "1  osm rail entrance, exit, station  \n",
      "2             gtfs rail stop points  \n",
      "3             gtfs rail stop points  \n",
      "4             gtfs rail stop points  \n"
     ]
    }
   ],
   "source": [
    "# Count missing values per row\n",
    "df['missing_count'] = df.isnull().sum(axis=1)\n",
    "\n",
    "# Sort by missing count (ascending), so best rows come first\n",
    "df_sorted = df.sort_values(by='missing_count')\n",
    "\n",
    "df_best_rows = df_sorted.drop_duplicates(subset='name', keep='first').copy()\n",
    "df_best_rows.drop(columns='missing_count', inplace=True)\n",
    "df_best_rows.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Convert specified columns to string\n",
    "df_best_rows['name'] = df_best_rows['name'].astype(str)\n",
    "df_best_rows['pin10'] = df_best_rows['pin10'].astype(str).str.zfill(10)  # pad to 10 digits\n",
    "df_best_rows['census_tract'] = df_best_rows['census_tract'].astype(str)\n",
    "df_best_rows['tract_geoid'] = df_best_rows['tract_geoid'].astype(str)\n",
    "df_best_rows['inclusion_source'] = df_best_rows['inclusion_source'].astype(str)\n",
    "\n",
    "# Fill nulls and 'nan' strings in these string columns\n",
    "string_cols = ['name', 'census_tract', 'tract_geoid', 'inclusion_source']\n",
    "df_best_rows[string_cols] = df_best_rows[string_cols].replace('nan', 'Unknown').fillna('Unknown')\n",
    "\n",
    "# Drop rows where 'name' is 'Unknown' (originally missing)\n",
    "df_best_rows = df_best_rows[df_best_rows['name'].str.lower() != 'unknown']\n",
    "\n",
    "# Reset index\n",
    "df_best_rows.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save cleaned file\n",
    "output_path = 'C:/Users/kaur6/Downloads/Urban Analytics/Parcel_CC_Cleaned.csv'\n",
    "df_best_rows.to_csv(output_path, index=False)\n",
    "\n",
    "# Preview\n",
    "print(\"‚úÖ Final cleaned file saved.\")\n",
    "print(df_best_rows.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6140fa27-7fce-4fbb-846a-330944d33428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Total duplicate rows based on 'name': 0\n"
     ]
    }
   ],
   "source": [
    "# Count total rows with duplicated 'name' values (excluding the first occurrence)\n",
    "duplicate_row_count = df_best_rows.duplicated(subset='name', keep='first').sum()\n",
    "print(f\"üîÅ Total duplicate rows based on 'name': {duplicate_row_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d61a2e7-9f17-487a-9060-2ec4db022f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************\n",
      "First 5 rows of the dataset:\n",
      "             name       pin10  job_no  longitude   latitude   census_tract  \\\n",
      "0  16052210320000  1605221032       0 -87.768517  41.904417  17031250600.0   \n",
      "1  16131260170000  1613126017       0 -87.701964  41.875518  17031271200.0   \n",
      "2  16243100090000  1624310009       0 -87.704923  41.852313  17031841700.0   \n",
      "3  16252180030000  1625218003       0 -87.692609  41.847523  17031301200.0   \n",
      "4  16123140420000  1612314042       0 -87.699154  41.884731  17031837100.0   \n",
      "\n",
      "     tract_geoid  tract_white_perc  tract_black_perc  \\\n",
      "0  17031250600.0          0.012864          0.863835   \n",
      "1  17031271200.0          0.059382          0.893112   \n",
      "2  17031841700.0          0.050959          0.438904   \n",
      "3  17031301200.0          0.031319          0.066308   \n",
      "4  17031837100.0          0.181078          0.652256   \n",
      "\n",
      "                   inclusion_source  \n",
      "0                   cdot bus routes  \n",
      "1  osm rail entrance, exit, station  \n",
      "2             gtfs rail stop points  \n",
      "3             gtfs rail stop points  \n",
      "4             gtfs rail stop points  \n",
      "**********************\n",
      "\n",
      "Dataset info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1416445 entries, 0 to 1416444\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Non-Null Count    Dtype  \n",
      "---  ------            --------------    -----  \n",
      " 0   name              1416445 non-null  object \n",
      " 1   pin10             1416445 non-null  int64  \n",
      " 2   job_no            1416445 non-null  int64  \n",
      " 3   longitude         1416445 non-null  float64\n",
      " 4   latitude          1416445 non-null  float64\n",
      " 5   census_tract      1416445 non-null  object \n",
      " 6   tract_geoid       1416445 non-null  object \n",
      " 7   tract_white_perc  1412494 non-null  float64\n",
      " 8   tract_black_perc  1412494 non-null  float64\n",
      " 9   inclusion_source  1416445 non-null  object \n",
      "dtypes: float64(4), int64(2), object(4)\n",
      "memory usage: 108.1+ MB\n",
      "None\n",
      "**********************\n",
      "\n",
      "Missing values per column:\n",
      "name                   0\n",
      "pin10                  0\n",
      "job_no                 0\n",
      "longitude              0\n",
      "latitude               0\n",
      "census_tract           0\n",
      "tract_geoid            0\n",
      "tract_white_perc    3951\n",
      "tract_black_perc    3951\n",
      "inclusion_source       0\n",
      "dtype: int64\n",
      "**********************\n",
      "\n",
      "Summary statistics:\n",
      "                  name         pin10        job_no     longitude  \\\n",
      "count          1416445  1.416445e+06  1.416445e+06  1.416445e+06   \n",
      "unique         1416445           NaN           NaN           NaN   \n",
      "top     28224040250000           NaN           NaN           NaN   \n",
      "freq                 1           NaN           NaN           NaN   \n",
      "mean               NaN  1.726120e+09  1.518080e+08 -8.776403e+01   \n",
      "std                NaN  7.943868e+08  5.308298e+08  1.404072e-01   \n",
      "min                NaN  1.011000e+08  0.000000e+00 -8.826351e+01   \n",
      "25%                NaN  1.225122e+09  0.000000e+00 -8.782874e+01   \n",
      "50%                NaN  1.632304e+09  0.000000e+00 -8.773560e+01   \n",
      "75%                NaN  2.412300e+09  0.000000e+00 -8.766786e+01   \n",
      "max                NaN  3.332999e+09  2.022001e+09 -8.752457e+01   \n",
      "\n",
      "            latitude   census_tract    tract_geoid  tract_white_perc  \\\n",
      "count   1.416445e+06        1416445        1416445      1.412494e+06   \n",
      "unique           NaN           1325           1319               NaN   \n",
      "top              NaN  17031824300.0  17031824300.0               NaN   \n",
      "freq             NaN           4616           4613               NaN   \n",
      "mean    4.184131e+01            NaN            NaN      4.229396e-01   \n",
      "std     1.679156e-01            NaN            NaN      3.143635e-01   \n",
      "min     4.146976e+01            NaN            NaN      0.000000e+00   \n",
      "25%     4.171641e+01            NaN            NaN      8.512347e-02   \n",
      "50%     4.184915e+01            NaN            NaN      4.475062e-01   \n",
      "75%     4.198037e+01            NaN            NaN      7.198891e-01   \n",
      "max     4.215407e+01            NaN            NaN      9.637121e-01   \n",
      "\n",
      "        tract_black_perc inclusion_source  \n",
      "count       1.412494e+06          1416445  \n",
      "unique               NaN                4  \n",
      "top                  NaN          Unknown  \n",
      "freq                 NaN           922496  \n",
      "mean        2.706242e-01              NaN  \n",
      "std         3.568083e-01              NaN  \n",
      "min         0.000000e+00              NaN  \n",
      "25%         1.482430e-02              NaN  \n",
      "50%         4.842181e-02              NaN  \n",
      "75%         5.549536e-01              NaN  \n",
      "max         1.000000e+00              NaN  \n",
      "**********************\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "file_path = 'C:/Users/kaur6/Downloads/Urban Analytics/Parcel_CC_Cleaned.csv'\n",
    "df = pd.read_csv(file_path, low_memory=False)\n",
    "print(\"**********************\")\n",
    "# Preview the data\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "print(\"**********************\")\n",
    "# Summary info\n",
    "print(\"\\nDataset info:\")\n",
    "print(df.info())\n",
    "print(\"**********************\")\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"**********************\")\n",
    "# Summary statistics\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(df.describe(include='all'))\n",
    "print(\"**********************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cad3cb-5231-4fcd-86a5-ee12ed9833bc",
   "metadata": {},
   "source": [
    "### renamed Parcel_CC_Cleaned.csv to Parcel_CC_Unique.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714daa52-e0f9-4848-bce1-72a19efa00d2",
   "metadata": {},
   "source": [
    "## Cook County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f721420-8ebd-494a-91b9-1c230fbda853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              pin  tax_year  card_num  class  township_code  \\\n",
      "0  31074070140000      2021         1  '295'             32   \n",
      "1  31074070140000      2022         1  '295'             32   \n",
      "2  31074070140000      2023         1  '295'             32   \n",
      "3  31074070140000      2024         1  '295'             32   \n",
      "4  31074070160000      2003         1  '295'             32   \n",
      "\n",
      "   proration_key_pin  pin_proration_rate  card_proration_rate cdu  \\\n",
      "0                NaN                   1                    0  AV   \n",
      "1                NaN                   1                    0  AV   \n",
      "2                NaN                   1                    0  AV   \n",
      "3                NaN                   1                    0  AV   \n",
      "4                NaN                   1                    0  AV   \n",
      "\n",
      "   pin_is_multicard  ...  basement_finish      roof_material  \\\n",
      "0             False  ...       Unfinished  Shingle + Asphalt   \n",
      "1             False  ...       Unfinished  Shingle + Asphalt   \n",
      "2             False  ...       Unfinished  Shingle + Asphalt   \n",
      "3             False  ...       Unfinished  Shingle + Asphalt   \n",
      "4              True  ...       Unfinished  Shingle + Asphalt   \n",
      "\n",
      "   single_v_multi_family      site_desirability  num_commercial_units  \\\n",
      "0          Single-Family  Not Relevant To Value                     0   \n",
      "1          Single-Family  Not Relevant To Value                     0   \n",
      "2          Single-Family  Not Relevant To Value                     0   \n",
      "3          Single-Family  Not Relevant To Value                     0   \n",
      "4          Single-Family  Not Relevant To Value                     0   \n",
      "\n",
      "   renovation  recent_renovation    porch  central_air  design_plan  \n",
      "0          No              False  Unknown  Central A/C   Stock Plan  \n",
      "1          No              False  Unknown  Central A/C   Stock Plan  \n",
      "2          No              False  Unknown  Central A/C   Stock Plan  \n",
      "3          No              False  Unknown  Central A/C   Stock Plan  \n",
      "4          No              False  Unknown  Central A/C   Stock Plan  \n",
      "\n",
      "[5 rows x 44 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 44 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   pin                       100 non-null    int64  \n",
      " 1   tax_year                  100 non-null    int64  \n",
      " 2   card_num                  100 non-null    int64  \n",
      " 3   class                     100 non-null    object \n",
      " 4   township_code             100 non-null    int64  \n",
      " 5   proration_key_pin         0 non-null      float64\n",
      " 6   pin_proration_rate        100 non-null    int64  \n",
      " 7   card_proration_rate       100 non-null    int64  \n",
      " 8   cdu                       100 non-null    object \n",
      " 9   pin_is_multicard          100 non-null    bool   \n",
      " 10  pin_num_cards             100 non-null    int64  \n",
      " 11  pin_is_multiland          100 non-null    bool   \n",
      " 12  pin_num_landlines         100 non-null    int64  \n",
      " 13  year_built                100 non-null    int64  \n",
      " 14  building_sqft             100 non-null    int64  \n",
      " 15  land_sqft                 100 non-null    int64  \n",
      " 16  num_bedrooms              100 non-null    int64  \n",
      " 17  num_rooms                 100 non-null    int64  \n",
      " 18  num_full_baths            100 non-null    int64  \n",
      " 19  num_half_baths            100 non-null    int64  \n",
      " 20  num_fireplaces            100 non-null    int64  \n",
      " 21  type_of_residence         100 non-null    object \n",
      " 22  construction_quality      100 non-null    object \n",
      " 23  num_apartments            100 non-null    int64  \n",
      " 24  attic_finish              100 non-null    object \n",
      " 25  garage_attached           100 non-null    object \n",
      " 26  garage_area_included      100 non-null    object \n",
      " 27  garage_size               100 non-null    object \n",
      " 28  garage_ext_wall_material  100 non-null    object \n",
      " 29  attic_type                100 non-null    object \n",
      " 30  basement_type             100 non-null    object \n",
      " 31  ext_wall_material         100 non-null    object \n",
      " 32  central_heating           100 non-null    object \n",
      " 33  repair_condition          100 non-null    object \n",
      " 34  basement_finish           100 non-null    object \n",
      " 35  roof_material             100 non-null    object \n",
      " 36  single_v_multi_family     100 non-null    object \n",
      " 37  site_desirability         100 non-null    object \n",
      " 38  num_commercial_units      100 non-null    int64  \n",
      " 39  renovation                100 non-null    object \n",
      " 40  recent_renovation         100 non-null    bool   \n",
      " 41  porch                     100 non-null    object \n",
      " 42  central_air               100 non-null    object \n",
      " 43  design_plan               100 non-null    object \n",
      "dtypes: bool(3), float64(1), int64(18), object(22)\n",
      "memory usage: 32.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "file_path = 'C:/Users/kaur6/Downloads/Urban Analytics/final_cook_county_data_with_same_dtype.csv'\n",
    "\n",
    "# Preview first 100 rows only\n",
    "df_sample = pd.read_csv(file_path, nrows=100, low_memory=False)\n",
    "print(df_sample.head())\n",
    "print(df_sample.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fef597f6-476a-406e-b8b0-b2c0635f3dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Processing chunk 1\n",
      "üîÑ Processing chunk 2\n",
      "üîÑ Processing chunk 3\n",
      "üîÑ Processing chunk 4\n",
      "üîÑ Processing chunk 5\n",
      "üîÑ Processing chunk 6\n",
      "üîÑ Processing chunk 7\n",
      "üîÑ Processing chunk 8\n",
      "üîÑ Processing chunk 9\n",
      "üîÑ Processing chunk 10\n",
      "üîÑ Processing chunk 11\n",
      "üîÑ Processing chunk 12\n",
      "üîÑ Processing chunk 13\n",
      "üîÑ Processing chunk 14\n",
      "üîÑ Processing chunk 15\n",
      "üîÑ Processing chunk 16\n",
      "üîÑ Processing chunk 17\n",
      "üîÑ Processing chunk 18\n",
      "üîÑ Processing chunk 19\n",
      "üîÑ Processing chunk 20\n",
      "üîÑ Processing chunk 21\n",
      "üîÑ Processing chunk 22\n",
      "üîÑ Processing chunk 23\n",
      "üîÑ Processing chunk 24\n",
      "üîÑ Processing chunk 25\n",
      "üîÑ Processing chunk 26\n",
      "üîÑ Processing chunk 27\n",
      "üîÑ Processing chunk 28\n",
      "üîÑ Processing chunk 29\n",
      "üîÑ Processing chunk 30\n",
      "üîÑ Processing chunk 31\n",
      "üîÑ Processing chunk 32\n",
      "üîÑ Processing chunk 33\n",
      "üîÑ Processing chunk 34\n",
      "üîÑ Processing chunk 35\n",
      "üîÑ Processing chunk 36\n",
      "üîÑ Processing chunk 37\n",
      "üîÑ Processing chunk 38\n",
      "üîÑ Processing chunk 39\n",
      "üîÑ Processing chunk 40\n",
      "üîÑ Processing chunk 41\n",
      "üîÑ Processing chunk 42\n",
      "üîÑ Processing chunk 43\n",
      "üîÑ Processing chunk 44\n",
      "üîÑ Processing chunk 45\n",
      "üîÑ Processing chunk 46\n",
      "üîÑ Processing chunk 47\n",
      "üîÑ Processing chunk 48\n",
      "üîÑ Processing chunk 49\n",
      "üîÑ Processing chunk 50\n",
      "üîÑ Processing chunk 51\n",
      "üîÑ Processing chunk 52\n",
      "üîÑ Processing chunk 53\n",
      "üîÑ Processing chunk 54\n",
      "üîÑ Processing chunk 55\n",
      "üîÑ Processing chunk 56\n",
      "üîÑ Processing chunk 57\n",
      "üîÑ Processing chunk 58\n",
      "üîÑ Processing chunk 59\n",
      "üîÑ Processing chunk 60\n",
      "üîÑ Processing chunk 61\n",
      "üîÑ Processing chunk 62\n",
      "üîÑ Processing chunk 63\n",
      "üîÑ Processing chunk 64\n",
      "üîÑ Processing chunk 65\n",
      "üîÑ Processing chunk 66\n",
      "üîÑ Processing chunk 67\n",
      "üîÑ Processing chunk 68\n",
      "üîÑ Processing chunk 69\n",
      "üîÑ Processing chunk 70\n",
      "üîÑ Processing chunk 71\n",
      "üîÑ Processing chunk 72\n",
      "üîÑ Processing chunk 73\n",
      "üîÑ Processing chunk 74\n",
      "üîÑ Processing chunk 75\n",
      "üîÑ Processing chunk 76\n",
      "üîÑ Processing chunk 77\n",
      "üîÑ Processing chunk 78\n",
      "üîÑ Processing chunk 79\n",
      "üîÑ Processing chunk 80\n",
      "üîÑ Processing chunk 81\n",
      "üîÑ Processing chunk 82\n",
      "üîÑ Processing chunk 83\n",
      "üîÑ Processing chunk 84\n",
      "üîÑ Processing chunk 85\n",
      "üîÑ Processing chunk 86\n",
      "üîÑ Processing chunk 87\n",
      "üîÑ Processing chunk 88\n",
      "üîÑ Processing chunk 89\n",
      "üîÑ Processing chunk 90\n",
      "üîÑ Processing chunk 91\n",
      "üîÑ Processing chunk 92\n",
      "üîÑ Processing chunk 93\n",
      "üîÑ Processing chunk 94\n",
      "üîÑ Processing chunk 95\n",
      "üîÑ Processing chunk 96\n",
      "üîÑ Processing chunk 97\n",
      "üîÑ Processing chunk 98\n",
      "üîÑ Processing chunk 99\n",
      "üîÑ Processing chunk 100\n",
      "üîÑ Processing chunk 101\n",
      "üîÑ Processing chunk 102\n",
      "üîÑ Processing chunk 103\n",
      "üîÑ Processing chunk 104\n",
      "üîÑ Processing chunk 105\n",
      "üîÑ Processing chunk 106\n",
      "üîÑ Processing chunk 107\n",
      "üîÑ Processing chunk 108\n",
      "üîÑ Processing chunk 109\n",
      "üîÑ Processing chunk 110\n",
      "üîÑ Processing chunk 111\n",
      "üîÑ Processing chunk 112\n",
      "üîÑ Processing chunk 113\n",
      "üîÑ Processing chunk 114\n",
      "üîÑ Processing chunk 115\n",
      "üîÑ Processing chunk 116\n",
      "üîÑ Processing chunk 117\n",
      "üîÑ Processing chunk 118\n",
      "üîÑ Processing chunk 119\n",
      "üîÑ Processing chunk 120\n",
      "üîÑ Processing chunk 121\n",
      "üîÑ Processing chunk 122\n",
      "üîÑ Processing chunk 123\n",
      "üîÑ Processing chunk 124\n",
      "üîÑ Processing chunk 125\n",
      "üîÑ Processing chunk 126\n",
      "üîÑ Processing chunk 127\n",
      "üîÑ Processing chunk 128\n",
      "üîÑ Processing chunk 129\n",
      "üîÑ Processing chunk 130\n",
      "üîÑ Processing chunk 131\n",
      "üîÑ Processing chunk 132\n",
      "üîÑ Processing chunk 133\n",
      "üîÑ Processing chunk 134\n",
      "üîÑ Processing chunk 135\n",
      "üîÑ Processing chunk 136\n",
      "üîÑ Processing chunk 137\n",
      "üîÑ Processing chunk 138\n",
      "üîÑ Processing chunk 139\n",
      "üîÑ Processing chunk 140\n",
      "üîÑ Processing chunk 141\n",
      "üîÑ Processing chunk 142\n",
      "üîÑ Processing chunk 143\n",
      "üîÑ Processing chunk 144\n",
      "üîÑ Processing chunk 145\n",
      "üîÑ Processing chunk 146\n",
      "üîÑ Processing chunk 147\n",
      "üîÑ Processing chunk 148\n",
      "üîÑ Processing chunk 149\n",
      "üîÑ Processing chunk 150\n",
      "üîÑ Processing chunk 151\n",
      "üîÑ Processing chunk 152\n",
      "üîÑ Processing chunk 153\n",
      "üîÑ Processing chunk 154\n",
      "üîÑ Processing chunk 155\n",
      "üîÑ Processing chunk 156\n",
      "üîÑ Processing chunk 157\n",
      "üîÑ Processing chunk 158\n",
      "üîÑ Processing chunk 159\n",
      "üîÑ Processing chunk 160\n",
      "üîÑ Processing chunk 161\n",
      "üîÑ Processing chunk 162\n",
      "üîÑ Processing chunk 163\n",
      "üîÑ Processing chunk 164\n",
      "üîÑ Processing chunk 165\n",
      "üîÑ Processing chunk 166\n",
      "üîÑ Processing chunk 167\n",
      "üîÑ Processing chunk 168\n",
      "üîÑ Processing chunk 169\n",
      "üîÑ Processing chunk 170\n",
      "üîÑ Processing chunk 171\n",
      "üîÑ Processing chunk 172\n",
      "üîÑ Processing chunk 173\n",
      "üîÑ Processing chunk 174\n",
      "üîÑ Processing chunk 175\n",
      "üîÑ Processing chunk 176\n",
      "üîÑ Processing chunk 177\n",
      "üîÑ Processing chunk 178\n",
      "üîÑ Processing chunk 179\n",
      "üîÑ Processing chunk 180\n",
      "üîÑ Processing chunk 181\n",
      "üîÑ Processing chunk 182\n",
      "üîÑ Processing chunk 183\n",
      "üîÑ Processing chunk 184\n",
      "üîÑ Processing chunk 185\n",
      "üîÑ Processing chunk 186\n",
      "üîÑ Processing chunk 187\n",
      "üîÑ Processing chunk 188\n",
      "üîÑ Processing chunk 189\n",
      "üîÑ Processing chunk 190\n",
      "üîÑ Processing chunk 191\n",
      "üîÑ Processing chunk 192\n",
      "üîÑ Processing chunk 193\n",
      "üîÑ Processing chunk 194\n",
      "üîÑ Processing chunk 195\n",
      "üîÑ Processing chunk 196\n",
      "üîÑ Processing chunk 197\n",
      "üîÑ Processing chunk 198\n",
      "üîÑ Processing chunk 199\n",
      "üîÑ Processing chunk 200\n",
      "üîÑ Processing chunk 201\n",
      "üîÑ Processing chunk 202\n",
      "üîÑ Processing chunk 203\n",
      "üîÑ Processing chunk 204\n",
      "üîÑ Processing chunk 205\n",
      "üîÑ Processing chunk 206\n",
      "üîÑ Processing chunk 207\n",
      "üîÑ Processing chunk 208\n",
      "üîÑ Processing chunk 209\n",
      "üîÑ Processing chunk 210\n",
      "üîÑ Processing chunk 211\n",
      "üîÑ Processing chunk 212\n",
      "üîÑ Processing chunk 213\n",
      "üîÑ Processing chunk 214\n",
      "üîÑ Processing chunk 215\n",
      "üîÑ Processing chunk 216\n",
      "üîÑ Processing chunk 217\n",
      "üîÑ Processing chunk 218\n",
      "üîÑ Processing chunk 219\n",
      "üîÑ Processing chunk 220\n",
      "üîÑ Processing chunk 221\n",
      "üîÑ Processing chunk 222\n",
      "üîÑ Processing chunk 223\n",
      "üîÑ Processing chunk 224\n",
      "üîÑ Processing chunk 225\n",
      "üîÑ Processing chunk 226\n",
      "üîÑ Processing chunk 227\n",
      "üîÑ Processing chunk 228\n",
      "üîÑ Processing chunk 229\n",
      "üîÑ Processing chunk 230\n",
      "üîÑ Processing chunk 231\n",
      "üîÑ Processing chunk 232\n",
      "üîÑ Processing chunk 233\n",
      "üîÑ Processing chunk 234\n",
      "üîÑ Processing chunk 235\n",
      "üîÑ Processing chunk 236\n",
      "üîÑ Processing chunk 237\n",
      "üîÑ Processing chunk 238\n",
      "üîÑ Processing chunk 239\n",
      "üîÑ Processing chunk 240\n",
      "üîÑ Processing chunk 241\n",
      "üîÑ Processing chunk 242\n",
      "üîÑ Processing chunk 243\n",
      "üîÑ Processing chunk 244\n",
      "üîÑ Processing chunk 245\n",
      "üîÑ Processing chunk 246\n",
      "üîÑ Processing chunk 247\n",
      "üîÑ Processing chunk 248\n",
      "üîÑ Processing chunk 249\n",
      "üîÑ Processing chunk 250\n",
      "üîÑ Processing chunk 251\n",
      "üîÑ Processing chunk 252\n",
      "üîÑ Processing chunk 253\n",
      "üîÑ Processing chunk 254\n",
      "üîÑ Processing chunk 255\n",
      "üîÑ Processing chunk 256\n",
      "üîÑ Processing chunk 257\n",
      "üîÑ Processing chunk 258\n",
      "üîÑ Processing chunk 259\n",
      "üîÑ Processing chunk 260\n",
      "üîÑ Processing chunk 261\n",
      "üîÑ Processing chunk 262\n",
      "üîÑ Processing chunk 263\n",
      "üîÑ Processing chunk 264\n",
      "üîÑ Processing chunk 265\n",
      "üîÑ Processing chunk 266\n",
      "üîÑ Processing chunk 267\n",
      "üîÑ Processing chunk 268\n",
      "üîÑ Processing chunk 269\n",
      "üîÑ Processing chunk 270\n",
      "üîÑ Processing chunk 271\n",
      "üîÑ Processing chunk 272\n",
      "üîÑ Processing chunk 273\n",
      "üîÑ Processing chunk 274\n",
      "üîÑ Processing chunk 275\n",
      "üîÑ Processing chunk 276\n",
      "üîÑ Processing chunk 277\n",
      "üîÑ Processing chunk 278\n",
      "üîÑ Processing chunk 279\n",
      "üîÑ Processing chunk 280\n",
      "üîÑ Processing chunk 281\n",
      "üîÑ Processing chunk 282\n",
      "üîÑ Processing chunk 283\n",
      "üîÑ Processing chunk 284\n",
      "üîÑ Processing chunk 285\n",
      "üîÅ Final deduplication across all chunks...\n",
      "‚úÖ Done! Final cleaned file saved to:\n",
      "C:/Users/kaur6/Downloads/Urban Analytics/cook_county_latest_by_pin.csv\n",
      "üìä Final row count: 1139209\n"
     ]
    }
   ],
   "source": [
    "file_path = 'C:/Users/kaur6/Downloads/Urban Analytics/final_cook_county_data_with_same_dtype.csv'\n",
    "intermediate_path = 'C:/Users/kaur6/Downloads/Urban Analytics/intermediate_latest_by_pin.csv'\n",
    "final_output = 'C:/Users/kaur6/Downloads/Urban Analytics/cook_county_latest_by_pin.csv'\n",
    "\n",
    "chunk_size = 100000\n",
    "first_chunk = True\n",
    "\n",
    "# Step 1: Chunked processing and intermediate writing\n",
    "for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunk_size, low_memory=False, on_bad_lines='skip')):\n",
    "    print(f\"üîÑ Processing chunk {i + 1}\")\n",
    "    \n",
    "    chunk['pin'] = chunk['pin'].astype(str)\n",
    "    chunk['tax_year'] = pd.to_numeric(chunk['tax_year'], errors='coerce')\n",
    "    chunk = chunk.dropna(subset=['tax_year'])\n",
    "\n",
    "    # Keep latest tax_year per pin in this chunk\n",
    "    chunk_latest = chunk.sort_values('tax_year').drop_duplicates('pin', keep='last')\n",
    "\n",
    "    # Write to intermediate file\n",
    "    mode = 'w' if first_chunk else 'a'\n",
    "    header = first_chunk\n",
    "    chunk_latest.to_csv(intermediate_path, mode=mode, header=header, index=False)\n",
    "    first_chunk = False\n",
    "\n",
    "# Step 2: Final deduplication on intermediate file\n",
    "print(\"üîÅ Final deduplication across all chunks...\")\n",
    "df = pd.read_csv(intermediate_path, low_memory=False)\n",
    "df['pin'] = df['pin'].astype(str)\n",
    "df['tax_year'] = pd.to_numeric(df['tax_year'], errors='coerce')\n",
    "df = df.sort_values('tax_year').drop_duplicates('pin', keep='last')\n",
    "\n",
    "# Step 3: Save final result\n",
    "df.to_csv(final_output, index=False)\n",
    "print(f\"‚úÖ Done! Final cleaned file saved to:\\n{final_output}\")\n",
    "print(\"üìä Final row count:\", df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "780b84cc-58d5-4554-bc45-e4977272bd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Total duplicate rows based on 'pin': 0\n"
     ]
    }
   ],
   "source": [
    "# Load the cleaned file\n",
    "final_path = 'C:/Users/kaur6/Downloads/Urban Analytics/cook_county_latest_by_pin.csv'\n",
    "df = pd.read_csv(final_path, low_memory=False)\n",
    "\n",
    "# Make sure 'pin' is treated as a string (to avoid issues with leading zeros)\n",
    "df['pin'] = df['pin'].astype(str)\n",
    "\n",
    "# Check for duplicate pin values\n",
    "duplicate_pins = df[df.duplicated(subset='pin', keep=False)]\n",
    "\n",
    "# Print summary\n",
    "print(f\"üîç Total duplicate rows based on 'pin': {duplicate_pins.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e467efe-b86e-4010-a071-7678e45a2b25",
   "metadata": {},
   "source": [
    "### renamed the file to cook_county_unique_pins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d1b164-54d1-4234-a024-dbfea68dc7a5",
   "metadata": {},
   "source": [
    "## Merged unique cook county pins with parcel_cc file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c8a2845-ea97-4411-a1b3-7c54178ab7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaur6\\AppData\\Local\\Temp\\ipykernel_26696\\2963591580.py:4: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df1 = pd.read_csv('C:/Users/kaur6/Downloads/Urban Analytics/cook_county_unique_pins.csv')\n",
      "C:\\Users\\kaur6\\AppData\\Local\\Temp\\ipykernel_26696\\2963591580.py:5: DtypeWarning: Columns (0,5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df2 = pd.read_csv('C:/Users/kaur6/Downloads/Urban Analytics/Parcel_CC_Unique.csv')\n"
     ]
    }
   ],
   "source": [
    "# Load both CSVs\n",
    "df1 = pd.read_csv('C:/Users/kaur6/Downloads/Urban Analytics/cook_county_unique_pins.csv')\n",
    "df2 = pd.read_csv('C:/Users/kaur6/Downloads/Urban Analytics/Parcel_CC_Unique.csv')\n",
    "\n",
    "# Rename 'name' in df2 to 'pin' for consistency\n",
    "df2.rename(columns={'name': 'pin'}, inplace=True)\n",
    "\n",
    "# Perform left join on 'pin'\n",
    "merged_df = pd.merge(df1, df2, on='pin', how='left')\n",
    "\n",
    "# Save result\n",
    "merged_df.to_csv('C:/Users/kaur6/Downloads/Urban Analytics/cook_county_merged_parcelCC.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3202288-df84-4162-83ee-63f6b0a071d6",
   "metadata": {},
   "source": [
    "## got only those pins where inclusion_source value is not null i.e. pins inside connected communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31285073-7fa8-4d24-a8c6-f28bc10ed68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV and force 'pin' column to be read as string\n",
    "df = pd.read_csv(\n",
    "    'C:/Users/kaur6/Downloads/Urban Analytics/cook_county_merged_parcelCC.csv',\n",
    "    dtype={'pin': str}, low_memory=False\n",
    ")\n",
    "\n",
    "# Filter: keep rows where inclusion_source is NOT NaN and NOT 'Unknown'\n",
    "filtered_df = df[df['inclusion_source'].notna() & (df['inclusion_source'] != 'Unknown')]\n",
    "\n",
    "# Save the filtered result ‚Äî PINs preserved as string\n",
    "filtered_df.to_csv('C:/Users/kaur6/Downloads/Urban Analytics/pins_inside_cc.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "739507b4-65af-4b6d-9739-46bcaa4dbce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique pins in cook county dataset: 1139209\n",
      "Number of pins in cook county (inside connected Communities): 212733\n"
     ]
    }
   ],
   "source": [
    "# Replace with your file path\n",
    "cook_county_pins = pd.read_csv('C:/Users/kaur6/Downloads/Urban Analytics/cook_county_unique_pins.csv', low_memory=False)\n",
    "# Print number of rows\n",
    "print(f\"Number of Unique pins in cook county dataset: {len(cook_county_pins)}\")\n",
    "\n",
    "# Replace with your file path\n",
    "cc_pins = pd.read_csv('C:/Users/kaur6/Downloads/Urban Analytics/pins_inside_cc.csv', low_memory=False)\n",
    "# Print number of rows\n",
    "print(f\"Number of pins in cook county (inside connected Communities): {len(cc_pins)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2330b846-35a4-4c59-98d2-82274a8445a3",
   "metadata": {},
   "source": [
    "## getting those pins where improvement is not done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec72cb8-865a-471b-a9fc-e9af9a405321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "# Path to your large file\n",
    "input_file = 'C:/Users/kaur6/Downloads/Urban Analytics/final_cook_county_data_with_same_dtype.csv'\n",
    "chunk_size = 1000000\n",
    "# Columns required for scoring (used inside is_improved), but we read all columns\n",
    "scoring_columns = [\n",
    "    'building_sqft',\n",
    "    'year_built',\n",
    "    'num_rooms',\n",
    "    'num_bedrooms',\n",
    "    'num_full_baths',\n",
    "    'type_of_residence',\n",
    "    'construction_quality'\n",
    "]\n",
    "\n",
    "vacant_rows = []\n",
    "chunk_num = 0\n",
    "\n",
    "def is_improved(row):\n",
    "    signals = [\n",
    "        not pd.isna(row['building_sqft']) and row['building_sqft'] >= 196,\n",
    "        not pd.isna(row['year_built']) and row['year_built'] > 0,\n",
    "        not pd.isna(row['num_rooms']) and row['num_rooms'] > 0,\n",
    "        not pd.isna(row['num_bedrooms']) and row['num_bedrooms'] > 0,\n",
    "        not pd.isna(row['num_full_baths']) and row['num_full_baths'] > 0,\n",
    "        pd.notna(row['type_of_residence']),\n",
    "        pd.notna(row['construction_quality'])\n",
    "    ]\n",
    "    return sum(signals) >= 3\n",
    "\n",
    "for chunk in pd.read_csv(input_file, chunksize=chunk_size, low_memory=False):\n",
    "    chunk_num += 1\n",
    "    print(f\"\\nüîÑ Processing chunk {chunk_num}...\")\n",
    "\n",
    "    # Filter using only scoring columns but keep all data\n",
    "    vacant_chunk = chunk[~chunk[scoring_columns].apply(is_improved, axis=1)]\n",
    "\n",
    "    vacant_rows.append(vacant_chunk)\n",
    "\n",
    "    print(f\"‚úÖ Finished chunk {chunk_num} | Vacant this chunk: {len(vacant_chunk)}\")\n",
    "\n",
    "# Combine and save\n",
    "vacant_df = pd.concat(vacant_rows, ignore_index=True)\n",
    "vacant_df.to_csv('C:/Users/kaur6/Downloads/Urban Analytics/parcels_no_improvement_full_columns.csv', index=False)\n",
    "\n",
    "print(f\"\\nüéØ Final: {len(vacant_df)} parcels classified as not clearly improved (with all original columns).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "639104fd-da8f-47a6-9167-ea710e4e3659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Done! Final rows with unique pins and latest tax_year: 18725\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV with all columns\n",
    "df = pd.read_csv('C:/Users/kaur6/Downloads/Urban Analytics/parcels_no_improvement_full_columns.csv', low_memory=False)\n",
    "\n",
    "# Ensure tax_year is numeric for proper comparison\n",
    "df['tax_year'] = pd.to_numeric(df['tax_year'], errors='coerce')\n",
    "\n",
    "# Drop rows with missing tax_year\n",
    "df = df.dropna(subset=['tax_year'])\n",
    "\n",
    "# Sort by pin and tax_year (latest first), then drop duplicates\n",
    "df_sorted = df.sort_values(by=['pin', 'tax_year'], ascending=[True, False])\n",
    "df_deduped = df_sorted.drop_duplicates(subset='pin', keep='first')\n",
    "\n",
    "# Save the cleaned version\n",
    "df_deduped.to_csv('C:/Users/kaur6/Downloads/Urban Analytics/parcels_no_improvement_unique_pin_latest_year.csv', index=False)\n",
    "\n",
    "print(f\"‚úÖ Done! Final rows with unique pins and latest tax_year: {len(df_deduped)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94958ec-723f-498e-8bd6-56330c0aafae",
   "metadata": {},
   "source": [
    "## Vacant lands owned by city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05af4184-4ec9-48fe-b362-4fd58bdbb7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyphens removed and dataset saved!\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset\n",
    "df = pd.read_csv(\"C:/Users/kaur6/Downloads/Urban Analytics/Cleaned_City_Owned_Land.csv\")\n",
    "\n",
    "# Remove hyphens from the 'PIN' column\n",
    "df['PIN'] = df['PIN'].str.replace('-', '', regex=True)\n",
    "\n",
    "# Save the cleaned dataset\n",
    "df.to_csv(\"C:/Users/kaur6/Downloads/Urban Analytics/Cleaned_City_Owned_Land_no_hyphen.csv\", index=False)\n",
    "\n",
    "print(\"Hyphens removed and dataset saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c3ef8c86-ae89-4980-bfa2-8e6606c3e2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pins not improved: 18725\n",
      "Number of pins in city owned land: 10916\n"
     ]
    }
   ],
   "source": [
    "not_imp_pins = pd.read_csv('C:/Users/kaur6/Downloads/Urban Analytics/parcels_no_improvement_unique_pin_latest_year.csv', dtype={'pin': str}, low_memory=False)\n",
    "# Print number of rows\n",
    "print(f\"Number of pins not improved: {len(not_imp_pins)}\")\n",
    "\n",
    "city_pins = pd.read_csv('C:/Users/kaur6/Downloads/Urban Analytics/Cleaned_City_Owned_Land_no_hyphen.csv', dtype={'PIN': str}, low_memory=False)\n",
    "# Print number of rows\n",
    "print(f\"Number of pins in city owned land: {len(city_pins)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2106216e-2bbb-4413-bb82-f0726614c9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique pins: 29511\n"
     ]
    }
   ],
   "source": [
    "city_pins.rename(columns={'PIN': 'pin'}, inplace=True)\n",
    "\n",
    "# Concatenate both DataFrames (only the 'pin' column from each)\n",
    "all_pins = pd.concat([not_imp_pins[['pin']], city_pins[['pin']]], ignore_index=True)\n",
    "\n",
    "# Drop duplicates\n",
    "unique_pins = all_pins.drop_duplicates()\n",
    "\n",
    "# Save to CSV\n",
    "unique_pins.to_csv('C:/Users/kaur6/Downloads/Urban Analytics/all_pins_vacant.csv', index=False)\n",
    "\n",
    "# Optional: print the number of unique pins\n",
    "print(f\"Total unique pins: {len(unique_pins)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f8db7996-e4c0-4527-935b-a092e54e6f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matching PINs: 7698\n"
     ]
    }
   ],
   "source": [
    "# Load both files\n",
    "df1 = pd.read_csv('C:/Users/kaur6/Downloads/Urban Analytics/pins_inside_cc.csv', dtype={'pin':str}, low_memory=False)\n",
    "df2 = pd.read_csv('C:/Users/kaur6/Downloads/Urban Analytics/all_pins_vacant.csv', dtype={'pin':str}, low_memory=False)\n",
    "\n",
    "# Make sure the column names are consistent\n",
    "# Assuming both files have a column named 'pin'\n",
    "common_pins = set(df1['pin']).intersection(set(df2['pin']))\n",
    "\n",
    "# Print the count of matching PINs\n",
    "print(f\"Number of matching PINs: {len(common_pins)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e4d1064c-bce1-4a17-a850-4e25448820b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows saved: 7698\n"
     ]
    }
   ],
   "source": [
    "# Filter rows from df1 where pin exists in df2\n",
    "filtered_df = df1[df1['pin'].isin(df2['pin'])]\n",
    "\n",
    "# Save the filtered result\n",
    "filtered_df.to_csv('C:/Users/kaur6/Downloads/Urban Analytics/pins_inside_cc_that_are_vacant.csv', index=False)\n",
    "\n",
    "# Optional: print how many matched rows are saved\n",
    "print(f\"Number of rows saved: {len(filtered_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8292b832-ddbd-42ef-988b-e731678b9933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pin                            0\n",
      "tax_year                       0\n",
      "card_num                       0\n",
      "class                          0\n",
      "township_code                  0\n",
      "proration_key_pin           6955\n",
      "pin_proration_rate             0\n",
      "card_proration_rate            0\n",
      "cdu                            0\n",
      "pin_is_multicard               0\n",
      "pin_num_cards                  0\n",
      "pin_is_multiland               0\n",
      "pin_num_landlines              0\n",
      "year_built                     0\n",
      "building_sqft                  0\n",
      "land_sqft                      0\n",
      "num_bedrooms                   0\n",
      "num_rooms                      0\n",
      "num_full_baths                 0\n",
      "num_half_baths                 0\n",
      "num_fireplaces                 0\n",
      "type_of_residence            378\n",
      "construction_quality           0\n",
      "num_apartments                 0\n",
      "attic_finish                   0\n",
      "garage_attached              448\n",
      "garage_area_included         449\n",
      "garage_size                    0\n",
      "garage_ext_wall_material       0\n",
      "attic_type                     0\n",
      "basement_type                385\n",
      "ext_wall_material            380\n",
      "central_heating                0\n",
      "repair_condition             383\n",
      "basement_finish                0\n",
      "roof_material                382\n",
      "single_v_multi_family        378\n",
      "site_desirability            383\n",
      "num_commercial_units           0\n",
      "renovation                   387\n",
      "recent_renovation              0\n",
      "porch                          0\n",
      "central_air                  381\n",
      "design_plan                    0\n",
      "pin10                          0\n",
      "job_no                         0\n",
      "longitude                      0\n",
      "latitude                       0\n",
      "census_tract                   0\n",
      "tract_geoid                    0\n",
      "tract_white_perc               0\n",
      "tract_black_perc               0\n",
      "inclusion_source               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Show count of null values in each column\n",
    "null_counts = filtered_df.isnull().sum()\n",
    "\n",
    "# Print the result\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16b8834f-e672-48a9-bfab-d10ffbfdcaa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Map with transit saved at: C:/Users/kaur6/Downloads/Urban Analytics/vacant_properties_with_transit_map.html\n"
     ]
    }
   ],
   "source": [
    "# --- Load pins ---\n",
    "pins_df = pd.read_csv(\"C:/Users/kaur6/Downloads/Urban Analytics/pins_inside_cc_that_are_vacant.csv\", dtype={'pin': str})\n",
    "pins_gdf = gpd.GeoDataFrame(\n",
    "    pins_df,\n",
    "    geometry=gpd.points_from_xy(pins_df['longitude'], pins_df['latitude']),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# --- Load Cook County block groups ---\n",
    "block_groups_gdf = gpd.read_file(\"C:/Users/kaur6/Downloads/Urban Analytics/cook_county_bg/cook_county_block_groups.shp\")\n",
    "block_groups_gdf = block_groups_gdf.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# --- Join pins with block groups ---\n",
    "joined = gpd.sjoin(pins_gdf, block_groups_gdf, how='inner', predicate='within')\n",
    "pin_counts = joined.groupby('GEOID').size().reset_index(name='pin_count')\n",
    "block_groups_gdf = block_groups_gdf.merge(pin_counts, on='GEOID', how='left')\n",
    "block_groups_gdf['pin_count'] = block_groups_gdf['pin_count'].fillna(0).astype(int)\n",
    "\n",
    "# --- Create base map ---\n",
    "m = folium.Map(location=[pins_df['latitude'].mean(), pins_df['longitude'].mean()], zoom_start=10, tiles='cartodbpositron')\n",
    "\n",
    "# --- Choropleth for GEOID blocks ---\n",
    "folium.Choropleth(\n",
    "    geo_data=block_groups_gdf,\n",
    "    data=block_groups_gdf,\n",
    "    columns=[\"GEOID\", \"pin_count\"],\n",
    "    key_on=\"feature.properties.GEOID\",\n",
    "    fill_color=\"OrRd\",\n",
    "    fill_opacity=0.6,\n",
    "    line_opacity=0.2,\n",
    "    legend_name=\"Number of Pins\"\n",
    ").add_to(m)\n",
    "\n",
    "# --- Add GEOID tooltips ---\n",
    "for _, row in block_groups_gdf.iterrows():\n",
    "    tooltip = folium.Tooltip(f\"GEOID: {row['GEOID']}<br>Pin Count: {row['pin_count']}\")\n",
    "    folium.GeoJson(\n",
    "        row['geometry'],\n",
    "        tooltip=tooltip,\n",
    "        style_function=lambda x: {\n",
    "            'color': 'black',\n",
    "            'weight': 0.4,\n",
    "            'fillOpacity': 0\n",
    "        }\n",
    "    ).add_to(m)\n",
    "\n",
    "# --- Add PIN markers (Blue) ---\n",
    "for _, row in joined.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        radius=2,\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_opacity=0.7,\n",
    "        tooltip=f\"PIN: {row['pin']}<br>GEOID: {row['GEOID']}\"\n",
    "    ).add_to(m)\n",
    "\n",
    "\n",
    "# --- Load bus routes CSV ---\n",
    "bus_routes_df = pd.read_csv(\"C:/Users/kaur6/Downloads/Urban Analytics/CTA_-_Bus_Routes_20250421.csv\")\n",
    "\n",
    "# --- Convert MULTILINESTRING to geometry ---\n",
    "bus_routes_df['geometry'] = bus_routes_df['the_geom'].apply(wkt.loads)\n",
    "\n",
    "# --- Convert to GeoDataFrame ---\n",
    "bus_routes_gdf = gpd.GeoDataFrame(bus_routes_df, geometry='geometry', crs=\"EPSG:4326\")\n",
    "\n",
    "# --- Plot bus routes on map ---\n",
    "for _, row in bus_routes_gdf.iterrows():\n",
    "    folium.GeoJson(\n",
    "        row['geometry'],\n",
    "        name=f\"Bus Route: {row['NAME']}\",\n",
    "        style_function=lambda x: {\n",
    "            'color': 'red',\n",
    "            'weight': 2,\n",
    "            'opacity': 0.6\n",
    "        },\n",
    "        tooltip=row['NAME']\n",
    "    ).add_to(m)\n",
    "\n",
    "# --- Load CTA L Stops ---\n",
    "l_stops = pd.read_csv(\"C:/Users/kaur6/Downloads/Urban Analytics/CTA_-_System_Information_-_List_of__L__Stops_20250421.csv\")\n",
    "for _, row in l_stops.iterrows():\n",
    "    try:\n",
    "        loc = ast.literal_eval(row['Location'])\n",
    "        folium.CircleMarker(\n",
    "            location=[loc[0], loc[1]],\n",
    "            radius=3,\n",
    "            color='darkgreen',\n",
    "            fill=True,\n",
    "            fill_color='darkgreen',\n",
    "            fill_opacity=0.6,\n",
    "            tooltip=row['STOP_NAME']\n",
    "        ).add_to(m)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "\n",
    "# --- Load Chicago boundary CSV ---\n",
    "chicago_boundary_df = pd.read_csv(\"C:/Users/kaur6/Downloads/Urban Analytics/chicago_boundary.csv\")\n",
    "\n",
    "# --- Convert geometry from WKT in 'the_geom' column ---\n",
    "from shapely import wkt\n",
    "chicago_boundary_df['geometry'] = chicago_boundary_df['the_geom'].apply(wkt.loads)\n",
    "\n",
    "# --- Create GeoDataFrame ---\n",
    "chicago_boundary_gdf = gpd.GeoDataFrame(chicago_boundary_df, geometry='geometry', crs=\"EPSG:4326\")\n",
    "\n",
    "# --- Add boundary to map ---\n",
    "folium.GeoJson(\n",
    "    chicago_boundary_gdf,\n",
    "    name=\"Chicago Boundary\",\n",
    "    style_function=lambda x: {\n",
    "        'color': 'black',\n",
    "        'weight': 2,\n",
    "        'fillOpacity': 0\n",
    "    },\n",
    "    tooltip=\"Chicago City Boundary\"\n",
    ").add_to(m)\n",
    "\n",
    "\n",
    "# --- Save Map ---\n",
    "output_path = \"C:/Users/kaur6/Downloads/Urban Analytics/vacant_properties_with_transit_map.html\"\n",
    "# --- Custom legend ---\n",
    "legend_html = \"\"\"\n",
    "<div style=\"\n",
    "    position: fixed;\n",
    "    bottom: 50px;\n",
    "    left: 50px;\n",
    "    width: 240px;\n",
    "    z-index: 9999;\n",
    "    font-size: 14px;\n",
    "    background-color: white;\n",
    "    border: 2px solid gray;\n",
    "    border-radius: 8px;\n",
    "    padding: 10px;\n",
    "    box-shadow: 2px 2px 6px rgba(0,0,0,0.3);\">\n",
    "    <strong>Map Legend</strong><br>\n",
    "    <svg width=\"20\" height=\"10\"><line x1=\"0\" y1=\"5\" x2=\"20\" y2=\"5\" style=\"stroke:red;stroke-width:2\" /></svg> Bus Routes<br>\n",
    "    <svg width=\"20\" height=\"10\"><line x1=\"0\" y1=\"5\" x2=\"20\" y2=\"5\" style=\"stroke:black;stroke-width:2\" /></svg> Chicago Boundary<br>\n",
    "    <i style=\"background: darkgreen; width: 10px; height: 10px; float: left; margin-right: 8px; border-radius: 50%;\"></i> CTA L Train Stations<br>\n",
    "    <i style=\"background: #ffeda0; width: 10px; height: 10px; float: left; margin-right: 8px;\"></i> Block Groups (Cook County)<br>\n",
    "    <div style=\"clear: both;\"></div>\n",
    "</div>\n",
    "\"\"\"\n",
    "m.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "m.save(output_path)\n",
    "print(f\"‚úÖ Map with transit saved at: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5377588-7b5e-4018-95e6-f52a9d070279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             pin  longitude  latitude\n",
      "0  1011000040000        NaN       NaN\n",
      "1  1011000060000        NaN       NaN\n",
      "2  1011000090000        NaN       NaN\n",
      "3  1011000370000        NaN       NaN\n",
      "4  1011000710000        NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "# Load the files\n",
    "df1 = pd.read_csv('C:/Users/kaur6/Downloads/Urban Analytics/Parcel_CC_Unique.csv', dtype={'name': str}, low_memory=False)\n",
    "df2 = pd.read_csv('C:/Users/kaur6/Downloads/Urban Analytics/all_pins_vacant.csv', dtype={'pin': str}, low_memory=False)\n",
    "\n",
    "# Merge to get 'tract_geoid' from df1 based on matching pin -> name\n",
    "merged_df = df2.merge(df1[['name', 'longitude', 'latitude']], left_on='pin', right_on='name', how='left')\n",
    "\n",
    "# If you only want 'pin' and 'tract_geoid' columns in the result:\n",
    "result_df = merged_df[['pin', 'longitude', 'latitude']]\n",
    "\n",
    "# Save to CSV if needed\n",
    "result_df.to_csv('C:/Users/kaur6/Downloads/Urban Analytics/all_pins_vacant_with_lat_long.csv', index=False)\n",
    "\n",
    "# View a sample\n",
    "print(result_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9716f8a-2d67-4514-a276-6757836815c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both CSV files\n",
    "all_pins_df = pd.read_csv(\"C:/Users/kaur6/Downloads/Urban Analytics/all_pins_vacant_with_lat_long.csv\", dtype={'pin': str}, low_memory=False)\n",
    "cc_pins_df = pd.read_csv(\"C:/Users/kaur6/Downloads/Urban Analytics/pins_inside_cc.csv\", dtype={'pin': str}, low_memory=False)\n",
    "\n",
    "# Create a set of pins that are inside connected communities\n",
    "cc_pins_set = set(cc_pins_df['pin'])\n",
    "\n",
    "# Add the column 'is_inside_cc' based on whether the pin is in the set\n",
    "all_pins_df['is_inside_cc'] = all_pins_df['pin'].apply(lambda x: 'yes' if x in cc_pins_set else 'no')\n",
    "\n",
    "# Optional: Save to a new file\n",
    "all_pins_df.to_csv(\"C:/Users/kaur6/Downloads/Urban Analytics/all_vacant_pins_with_cc_flag.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83376e40-eb52-4bc8-9aa7-b425f500d69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows where is_inside_cc is 'yes': 7698\n"
     ]
    }
   ],
   "source": [
    "# Load the file (make sure this is the file where 'is_inside_cc' has been added)\n",
    "df = pd.read_csv(\"C:/Users/kaur6/Downloads/Urban Analytics/all_vacant_pins_with_cc_flag.csv\", dtype={'pin': str})\n",
    "\n",
    "# Count rows where 'is_inside_cc' is 'yes'\n",
    "count_yes = (df['is_inside_cc'] == 'yes').sum()\n",
    "\n",
    "print(f\"Number of rows where is_inside_cc is 'yes': {count_yes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae602f18-0980-43e1-97c0-44d5ebeeb836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the pins CSV\n",
    "pins_df = pd.read_csv(\"C:/Users/kaur6/Downloads/Urban Analytics/all_vacant_pins_with_cc_flag.csv\")  # Update path if needed\n",
    "pins_gdf = gpd.GeoDataFrame(\n",
    "    pins_df,\n",
    "    geometry=gpd.points_from_xy(pins_df.longitude, pins_df.latitude),\n",
    "    crs=\"EPSG:4326\"  # WGS 84\n",
    ")\n",
    "\n",
    "# Step 2: Load the Cook County block groups shapefile\n",
    "block_groups_gdf = gpd.read_file(\"C:/Users/kaur6/Downloads/Urban Analytics/cook_county_bg/cook_county_block_groups.shp\")\n",
    "\n",
    "# Ensure both layers are in the same coordinate system\n",
    "block_groups_gdf = block_groups_gdf.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# Step 3: Spatial join - find which block group each pin falls into\n",
    "joined = gpd.sjoin(pins_gdf, block_groups_gdf, how=\"left\", predicate=\"within\")\n",
    "\n",
    "# Step 4: Save to new CSV with block group GEOID\n",
    "joined[['pin', 'longitude', 'latitude', 'is_inside_cc', 'GEOID']].to_csv(\"C:/Users/kaur6/Downloads/Urban Analytics/vacant_pins_with_block_group_cc_Flag.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8ef427-0391-4a19-97c6-b50d4d7df3f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (geo_env)",
   "language": "python",
   "name": "geo_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
