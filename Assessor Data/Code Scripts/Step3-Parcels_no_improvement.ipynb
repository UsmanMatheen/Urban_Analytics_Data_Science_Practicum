{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69ec8902-832c-464d-92e0-ea31532393d1",
   "metadata": {},
   "source": [
    "## Identifying Parcels with No Physical Improvements (Cook County)\n",
    "\n",
    "This notebook processes parcel-level data from Cook County to isolate PINs where **no improvements** have been made — such as the absence of buildings, construction, or renovations. These vacant or unimproved parcels are critical for identifying **underutilized land** for potential redevelopment, planning, or investment.\n",
    "\n",
    "### Data Source:\n",
    "- Dataset: `final_cook_county_data_with_same_dtype.csv`\n",
    "- This file contains parcel identifiers along with structural details like:\n",
    "  - `building_sqft`, `num_rooms`, `num_full_baths`, `garage_size`, `year_built`, etc.\n",
    "\n",
    "### Process Overview:\n",
    "1. Load parcel data in chunks to handle large size efficiently.\n",
    "2. Define rules for detecting unimproved parcels — typically rows where key structural fields are null or zero.\n",
    "3. Filter parcels that meet the “no improvement” criteria.\n",
    "4. Deduplicate by PIN, keeping only the latest available `tax_year`.\n",
    "5. Export a clean list of unimproved parcels for further use.\n",
    "\n",
    "### ✅ Output:\n",
    "- A cleaned CSV file containing **only unimproved parcels**, each represented by a unique PIN and its most recent tax record.\n",
    "- This dataset can be merged with other layers (e.g., Connected Communities, transit access, city-owned land) for policy analysis and development planning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6372f545-1472-4469-85a5-303b3fd5cfb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Processing chunk 1...\n",
      "✅ Finished chunk 1 | Vacant this chunk: 1342\n",
      "\n",
      "🔄 Processing chunk 2...\n",
      "✅ Finished chunk 2 | Vacant this chunk: 616\n",
      "\n",
      "🔄 Processing chunk 3...\n",
      "✅ Finished chunk 3 | Vacant this chunk: 387\n",
      "\n",
      "🔄 Processing chunk 4...\n",
      "✅ Finished chunk 4 | Vacant this chunk: 667\n",
      "\n",
      "🔄 Processing chunk 5...\n",
      "✅ Finished chunk 5 | Vacant this chunk: 344\n",
      "\n",
      "🔄 Processing chunk 6...\n",
      "✅ Finished chunk 6 | Vacant this chunk: 553\n",
      "\n",
      "🔄 Processing chunk 7...\n",
      "✅ Finished chunk 7 | Vacant this chunk: 748\n",
      "\n",
      "🔄 Processing chunk 8...\n",
      "✅ Finished chunk 8 | Vacant this chunk: 757\n",
      "\n",
      "🔄 Processing chunk 9...\n",
      "✅ Finished chunk 9 | Vacant this chunk: 1207\n",
      "\n",
      "🔄 Processing chunk 10...\n",
      "✅ Finished chunk 10 | Vacant this chunk: 1617\n",
      "\n",
      "🔄 Processing chunk 11...\n",
      "✅ Finished chunk 11 | Vacant this chunk: 3154\n",
      "\n",
      "🔄 Processing chunk 12...\n",
      "✅ Finished chunk 12 | Vacant this chunk: 5084\n",
      "\n",
      "🔄 Processing chunk 13...\n",
      "✅ Finished chunk 13 | Vacant this chunk: 7252\n",
      "\n",
      "🔄 Processing chunk 14...\n",
      "✅ Finished chunk 14 | Vacant this chunk: 1464\n",
      "\n",
      "🔄 Processing chunk 15...\n",
      "✅ Finished chunk 15 | Vacant this chunk: 4549\n",
      "\n",
      "🔄 Processing chunk 16...\n",
      "✅ Finished chunk 16 | Vacant this chunk: 4233\n",
      "\n",
      "🔄 Processing chunk 17...\n",
      "✅ Finished chunk 17 | Vacant this chunk: 9612\n",
      "\n",
      "🔄 Processing chunk 18...\n",
      "✅ Finished chunk 18 | Vacant this chunk: 1956\n",
      "\n",
      "🔄 Processing chunk 19...\n",
      "✅ Finished chunk 19 | Vacant this chunk: 2146\n",
      "\n",
      "🔄 Processing chunk 20...\n",
      "✅ Finished chunk 20 | Vacant this chunk: 2097\n",
      "\n",
      "🔄 Processing chunk 21...\n",
      "✅ Finished chunk 21 | Vacant this chunk: 2684\n",
      "\n",
      "🔄 Processing chunk 22...\n",
      "✅ Finished chunk 22 | Vacant this chunk: 1693\n",
      "\n",
      "🔄 Processing chunk 23...\n",
      "✅ Finished chunk 23 | Vacant this chunk: 1091\n",
      "\n",
      "🔄 Processing chunk 24...\n",
      "✅ Finished chunk 24 | Vacant this chunk: 1265\n",
      "\n",
      "🔄 Processing chunk 25...\n",
      "✅ Finished chunk 25 | Vacant this chunk: 1714\n",
      "\n",
      "🔄 Processing chunk 26...\n",
      "✅ Finished chunk 26 | Vacant this chunk: 1310\n",
      "\n",
      "🔄 Processing chunk 27...\n",
      "✅ Finished chunk 27 | Vacant this chunk: 840\n",
      "\n",
      "🔄 Processing chunk 28...\n",
      "✅ Finished chunk 28 | Vacant this chunk: 1845\n",
      "\n",
      "🔄 Processing chunk 29...\n",
      "✅ Finished chunk 29 | Vacant this chunk: 864\n",
      "\n",
      "🎯 Final: 63091 parcels classified as not clearly improved (with all original columns).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "# Path to your large file\n",
    "input_file = 'C:/Users/kaur6/Downloads/Urban Analytics/final_cook_county_data_with_same_dtype.csv'\n",
    "chunk_size = 1000000\n",
    "# Columns required for scoring (used inside is_improved), but we read all columns\n",
    "scoring_columns = [\n",
    "    'building_sqft',\n",
    "    'year_built',\n",
    "    'num_rooms',\n",
    "    'num_bedrooms',\n",
    "    'num_full_baths',\n",
    "    'type_of_residence',\n",
    "    'construction_quality'\n",
    "]\n",
    "\n",
    "vacant_rows = []\n",
    "chunk_num = 0\n",
    "\n",
    "def is_improved(row):\n",
    "    signals = [\n",
    "        not pd.isna(row['building_sqft']) and row['building_sqft'] >= 196,\n",
    "        not pd.isna(row['year_built']) and row['year_built'] > 0,\n",
    "        not pd.isna(row['num_rooms']) and row['num_rooms'] > 0,\n",
    "        not pd.isna(row['num_bedrooms']) and row['num_bedrooms'] > 0,\n",
    "        not pd.isna(row['num_full_baths']) and row['num_full_baths'] > 0,\n",
    "        pd.notna(row['type_of_residence']),\n",
    "        pd.notna(row['construction_quality'])\n",
    "    ]\n",
    "    return sum(signals) >= 3\n",
    "\n",
    "for chunk in pd.read_csv(input_file, chunksize=chunk_size, low_memory=False):\n",
    "    chunk_num += 1\n",
    "    print(f\"\\n🔄 Processing chunk {chunk_num}...\")\n",
    "\n",
    "    # Filter using only scoring columns but keep all data\n",
    "    vacant_chunk = chunk[~chunk[scoring_columns].apply(is_improved, axis=1)]\n",
    "\n",
    "    vacant_rows.append(vacant_chunk)\n",
    "\n",
    "    print(f\"✅ Finished chunk {chunk_num} | Vacant this chunk: {len(vacant_chunk)}\")\n",
    "\n",
    "# Combine and save\n",
    "vacant_df = pd.concat(vacant_rows, ignore_index=True)\n",
    "vacant_df.to_csv('C:/Users/kaur6/Downloads/Urban Analytics/parcels_no_improvement_full_columns.csv', index=False)\n",
    "\n",
    "print(f\"\\n🎯 Final: {len(vacant_df)} parcels classified as not clearly improved (with all original columns).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1415c596-09e4-4e22-bfec-7d3f16256308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaur6\\AppData\\Local\\Temp\\ipykernel_5956\\362984273.py:2: DtypeWarning: Columns (36) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('C:/Users/kaur6/Downloads/Urban Analytics/parcels_no_improvement_full_columns.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done! Final rows with unique pins and latest tax_year: 18725\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV with all columns\n",
    "df = pd.read_csv('C:/Users/kaur6/Downloads/Urban Analytics/parcels_no_improvement_full_columns.csv')\n",
    "\n",
    "# Ensure tax_year is numeric for proper comparison\n",
    "df['tax_year'] = pd.to_numeric(df['tax_year'], errors='coerce')\n",
    "\n",
    "# Drop rows with missing tax_year\n",
    "df = df.dropna(subset=['tax_year'])\n",
    "\n",
    "# Sort by pin and tax_year (latest first), then drop duplicates\n",
    "df_sorted = df.sort_values(by=['pin', 'tax_year'], ascending=[True, False])\n",
    "df_deduped = df_sorted.drop_duplicates(subset='pin', keep='first')\n",
    "\n",
    "# Save the cleaned version\n",
    "df_deduped.to_csv('C:/Users/kaur6/Downloads/Urban Analytics/parcels_no_improvement_unique_pin_latest_year.csv', index=False)\n",
    "\n",
    "print(f\"✅ Done! Final rows with unique pins and latest tax_year: {len(df_deduped)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20d09dd-5b60-43d4-8c5b-e45cdcdef5df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (geo_env)",
   "language": "python",
   "name": "geo_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
